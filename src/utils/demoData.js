// Realistic demo data for Mistral AI VoC Intelligence Tool
// 45 customer feedback items covering enterprise scenarios

export const DEMO_FEEDBACK = [
  // === FEATURE REQUESTS (12 items) ===
  {
    id: 'demo-1',
    timestamp: new Date(Date.now() - 2 * 24 * 60 * 60 * 1000).toISOString(),
    source: 'call_notes',
    raw_text: "QBR with TechCorp - They're scaling up their usage significantly. Currently processing 50M tokens/month but expect to hit 200M by Q2. Main ask: they need guaranteed latency SLAs for Mistral Large. Their production systems can't handle variability above 500ms p99. Also mentioned they evaluated GPT-4 but chose us for cost efficiency. Want to discuss enterprise pricing tiers.",
    category: 'feature_request',
    features_mentioned: ['latency SLAs', 'enterprise pricing', 'Mistral Large'],
    competitors_mentioned: ['OpenAI', 'GPT-4'],
    use_case: 'Production API integration at scale',
    sentiment: 'positive',
    urgency: 'high',
    summary: 'Enterprise customer scaling to 200M tokens needs latency SLAs and enterprise pricing',
    customer_name: 'TechCorp',
    arr_tier: 'enterprise',
    customer_health: 'healthy',
    strategic_value: 'lighthouse'
  },
  {
    id: 'demo-6',
    timestamp: new Date(Date.now() - 10 * 24 * 60 * 60 * 1000).toISOString(),
    source: 'qbr',
    raw_text: "Product feedback from QBR: Customer loves Le Chat for internal knowledge base queries. Requesting: 1) SSO integration for enterprise deployment, 2) Usage analytics dashboard, 3) Custom system prompts per workspace. Would increase seats from 50 to 500 if these ship.",
    category: 'feature_request',
    features_mentioned: ['Le Chat', 'SSO', 'analytics dashboard', 'custom prompts', 'workspaces'],
    competitors_mentioned: [],
    use_case: 'Enterprise knowledge base',
    sentiment: 'positive',
    urgency: 'medium',
    summary: 'Le Chat expansion opportunity: SSO, analytics, custom prompts could drive 10x seat growth',
    customer_name: 'GlobalCorp',
    arr_tier: 'enterprise',
    customer_health: 'healthy',
    strategic_value: 'lighthouse'
  },
  {
    id: 'demo-11',
    timestamp: new Date(Date.now() - 15 * 24 * 60 * 60 * 1000).toISOString(),
    source: 'call_notes',
    raw_text: "Discovery call with potential enterprise customer. They need vision capabilities for document processing - invoices, contracts, handwritten notes. Currently using GPT-4V but cost is prohibitive at their scale (processing 100K documents/month). Very interested in Pixtral pricing.",
    category: 'feature_request',
    features_mentioned: ['vision', 'Pixtral', 'document processing', 'OCR'],
    competitors_mentioned: ['OpenAI', 'GPT-4V'],
    use_case: 'Document processing at scale',
    sentiment: 'positive',
    urgency: 'medium',
    summary: 'New enterprise prospect interested in Pixtral for document processing, GPT-4V too expensive',
    customer_name: 'DocuProcess',
    arr_tier: 'enterprise',
    customer_health: 'healthy',
    strategic_value: 'lighthouse'
  },
  {
    id: 'demo-12',
    timestamp: new Date(Date.now() - 20 * 24 * 60 * 60 * 1000).toISOString(),
    source: 'support',
    raw_text: "Hitting context window limits with Mistral Large. Our legal team needs to process 200-page contracts but we're having to chunk and lose context. Is there a roadmap for longer context? GPT-4 Turbo supports 128K which would solve this.",
    category: 'feature_request',
    features_mentioned: ['context window', 'long documents', 'legal'],
    competitors_mentioned: ['OpenAI', 'GPT-4 Turbo'],
    use_case: 'Legal document analysis',
    sentiment: 'neutral',
    urgency: 'medium',
    summary: 'Customer needs longer context window for legal documents, comparing to GPT-4 Turbo 128K',
    customer_name: 'LegalTech Solutions',
    arr_tier: 'mid_market',
    customer_health: 'healthy',
    strategic_value: 'standard'
  },
  {
    id: 'demo-13',
    timestamp: new Date(Date.now() - 3 * 24 * 60 * 60 * 1000).toISOString(),
    source: 'email',
    raw_text: "We need batch processing capabilities with guaranteed turnaround times. Currently we're making thousands of individual API calls which is inefficient. OpenAI's batch API gives us 50% cost savings. Does Mistral have anything similar on the roadmap?",
    category: 'feature_request',
    features_mentioned: ['batch API', 'batch processing', 'cost optimization'],
    competitors_mentioned: ['OpenAI'],
    use_case: 'High-volume batch processing',
    sentiment: 'neutral',
    urgency: 'medium',
    summary: 'Customer requesting batch API similar to OpenAI for 50% cost savings',
    customer_name: 'DataPipeline Inc',
    arr_tier: 'mid_market',
    customer_health: 'healthy',
    strategic_value: 'standard'
  },
  {
    id: 'demo-14',
    timestamp: new Date(Date.now() - 8 * 24 * 60 * 60 * 1000).toISOString(),
    source: 'slack',
    raw_text: "Our security team is asking about SOC 2 Type II certification. We're a fintech and this is a hard requirement for us to expand usage. Also need a BAA for HIPAA compliance as we're exploring healthcare use cases.",
    category: 'feature_request',
    features_mentioned: ['SOC 2', 'HIPAA', 'BAA', 'compliance', 'security'],
    competitors_mentioned: [],
    use_case: 'Fintech and healthcare applications',
    sentiment: 'neutral',
    urgency: 'high',
    summary: 'Fintech customer needs SOC 2 Type II and BAA for HIPAA to expand usage',
    customer_name: 'FinSecure',
    arr_tier: 'enterprise',
    customer_health: 'healthy',
    strategic_value: 'lighthouse'
  },
  {
    id: 'demo-15',
    timestamp: new Date(Date.now() - 11 * 24 * 60 * 60 * 1000).toISOString(),
    source: 'call_notes',
    raw_text: "Customer building a RAG application and wants embedding models from Mistral. Currently using OpenAI's ada-002 but would prefer to consolidate vendors. Asked about Mistral embed models and vector database partnerships.",
    category: 'feature_request',
    features_mentioned: ['embeddings', 'RAG', 'vector database'],
    competitors_mentioned: ['OpenAI', 'ada-002'],
    use_case: 'RAG application',
    sentiment: 'positive',
    urgency: 'medium',
    summary: 'Customer wants Mistral embeddings to consolidate from OpenAI ada-002',
    customer_name: 'SearchAI',
    arr_tier: 'mid_market',
    customer_health: 'healthy',
    strategic_value: 'standard'
  },
  {
    id: 'demo-16',
    timestamp: new Date(Date.now() - 5 * 24 * 60 * 60 * 1000).toISOString(),
    source: 'support',
    raw_text: "Request for model customization options. We have domain-specific terminology (medical/pharma) that the base models struggle with. Fine-tuning access would be huge for us. Anthropic mentioned they're working on this.",
    category: 'feature_request',
    features_mentioned: ['fine-tuning', 'customization', 'domain-specific', 'medical'],
    competitors_mentioned: ['Anthropic'],
    use_case: 'Medical/pharma domain adaptation',
    sentiment: 'neutral',
    urgency: 'medium',
    summary: 'Pharma customer needs fine-tuning for domain-specific medical terminology',
    customer_name: 'PharmaTech',
    arr_tier: 'enterprise',
    customer_health: 'healthy',
    strategic_value: 'lighthouse'
  },
  {
    id: 'demo-17',
    timestamp: new Date(Date.now() - 14 * 24 * 60 * 60 * 1000).toISOString(),
    source: 'qbr',
    raw_text: "Enterprise customer requesting dedicated capacity. They want guaranteed throughput without sharing infrastructure. Willing to pay premium for isolated deployment. Current shared infrastructure causes unpredictable latency during peak hours.",
    category: 'feature_request',
    features_mentioned: ['dedicated capacity', 'isolated deployment', 'guaranteed throughput'],
    competitors_mentioned: [],
    use_case: 'Mission-critical production systems',
    sentiment: 'neutral',
    urgency: 'high',
    summary: 'Enterprise wants dedicated capacity with guaranteed throughput, willing to pay premium',
    customer_name: 'MegaCorp',
    arr_tier: 'enterprise',
    customer_health: 'healthy',
    strategic_value: 'lighthouse'
  },
  {
    id: 'demo-18',
    timestamp: new Date(Date.now() - 9 * 24 * 60 * 60 * 1000).toISOString(),
    source: 'email',
    raw_text: "Need real-time streaming with word-level timestamps for our transcription product. We're building live captioning and need to sync text with audio. Currently evaluating Assembly AI but prefer to use Mistral for consistency.",
    category: 'feature_request',
    features_mentioned: ['streaming', 'timestamps', 'transcription', 'live captioning'],
    competitors_mentioned: ['Assembly AI'],
    use_case: 'Live captioning and transcription',
    sentiment: 'positive',
    urgency: 'medium',
    summary: 'Customer needs streaming with timestamps for live captioning product',
    customer_name: 'CaptionPro',
    arr_tier: 'mid_market',
    customer_health: 'healthy',
    strategic_value: 'standard'
  },
  {
    id: 'demo-19',
    timestamp: new Date(Date.now() - 18 * 24 * 60 * 60 * 1000).toISOString(),
    source: 'call_notes',
    raw_text: "Multi-modal request: customer wants to process images alongside text in a single API call. Building a product catalog system that needs to understand product images and descriptions together. GPT-4V handles this well.",
    category: 'feature_request',
    features_mentioned: ['multi-modal', 'image processing', 'vision', 'product catalog'],
    competitors_mentioned: ['OpenAI', 'GPT-4V'],
    use_case: 'E-commerce product catalog',
    sentiment: 'neutral',
    urgency: 'medium',
    summary: 'E-commerce customer needs multi-modal API for product catalog with images',
    customer_name: 'ShopifyPlus Partner',
    arr_tier: 'mid_market',
    customer_health: 'healthy',
    strategic_value: 'standard'
  },
  {
    id: 'demo-20',
    timestamp: new Date(Date.now() - 22 * 24 * 60 * 60 * 1000).toISOString(),
    source: 'support',
    raw_text: "Requesting webhook support for async job completion. We submit long-running tasks and currently have to poll for results. A callback mechanism would simplify our architecture significantly.",
    category: 'feature_request',
    features_mentioned: ['webhooks', 'async', 'callbacks', 'long-running tasks'],
    competitors_mentioned: [],
    use_case: 'Async processing pipeline',
    sentiment: 'neutral',
    urgency: 'low',
    summary: 'Customer wants webhooks for async job completion instead of polling',
    customer_name: 'AsyncFlow',
    arr_tier: 'mid_market',
    customer_health: 'healthy',
    strategic_value: 'standard'
  },

  // === BUG REPORTS (8 items) ===
  {
    id: 'demo-2',
    timestamp: new Date(Date.now() - 5 * 24 * 60 * 60 * 1000).toISOString(),
    source: 'support',
    raw_text: "URGENT: Our batch processing jobs are failing intermittently. Getting 503 errors during peak hours (2-4pm UTC). This is blocking our daily report generation. We've tried implementing retries but the errors persist. Need immediate escalation - our CFO is asking questions about reliability.",
    category: 'bug',
    features_mentioned: ['batch processing', 'API reliability', 'rate limiting'],
    competitors_mentioned: [],
    use_case: 'Batch report generation',
    sentiment: 'frustrated',
    urgency: 'critical',
    summary: 'Critical API reliability issues during peak hours blocking production workflows',
    customer_name: 'FinanceAI',
    arr_tier: 'enterprise',
    customer_health: 'at_risk',
    strategic_value: 'lighthouse'
  },
  {
    id: 'demo-21',
    timestamp: new Date(Date.now() - 1 * 24 * 60 * 60 * 1000).toISOString(),
    source: 'support',
    raw_text: "Experiencing random timeouts on Mistral Large API calls. About 5% of requests are timing out after 30 seconds even for simple prompts. This started happening 3 days ago. Our monitoring shows no issues on our end.",
    category: 'bug',
    features_mentioned: ['timeouts', 'Mistral Large', 'API reliability'],
    competitors_mentioned: [],
    use_case: 'Production API usage',
    sentiment: 'frustrated',
    urgency: 'high',
    summary: '5% of API requests timing out on Mistral Large, started 3 days ago',
    customer_name: 'ReliableAI',
    arr_tier: 'mid_market',
    customer_health: 'at_risk',
    strategic_value: 'standard'
  },
  {
    id: 'demo-22',
    timestamp: new Date(Date.now() - 4 * 24 * 60 * 60 * 1000).toISOString(),
    source: 'slack',
    raw_text: "JSON mode is returning invalid JSON about 3% of the time. We're getting responses with trailing commas and unescaped characters. This is breaking our automated pipelines. Can you investigate?",
    category: 'bug',
    features_mentioned: ['JSON mode', 'structured output', 'parsing'],
    competitors_mentioned: [],
    use_case: 'Automated data extraction',
    sentiment: 'frustrated',
    urgency: 'high',
    summary: 'JSON mode returning invalid JSON 3% of the time, breaking automated pipelines',
    customer_name: 'DataExtract Co',
    arr_tier: 'mid_market',
    customer_health: 'at_risk',
    strategic_value: 'standard'
  },
  {
    id: 'demo-23',
    timestamp: new Date(Date.now() - 7 * 24 * 60 * 60 * 1000).toISOString(),
    source: 'email',
    raw_text: "The token counting in the API response doesn't match our calculations. We're being charged for more tokens than we're sending. Discrepancy is about 15-20% on average. Need clarification on tokenization.",
    category: 'bug',
    features_mentioned: ['token counting', 'billing', 'tokenization'],
    competitors_mentioned: [],
    use_case: 'Cost tracking and budgeting',
    sentiment: 'frustrated',
    urgency: 'medium',
    summary: 'Token count discrepancy of 15-20% between API response and customer calculations',
    customer_name: 'BudgetAI',
    arr_tier: 'mid_market',
    customer_health: 'healthy',
    strategic_value: 'standard'
  },
  {
    id: 'demo-7',
    timestamp: new Date(Date.now() - 4 * 24 * 60 * 60 * 1000).toISOString(),
    source: 'support',
    raw_text: "We're getting inconsistent results with code generation. Same prompt produces working Python code 80% of the time but the other 20% has syntax errors or missing imports. This is blocking our code review automation project. Codex and Copilot are more reliable for this use case.",
    category: 'bug',
    features_mentioned: ['code generation', 'Python', 'consistency'],
    competitors_mentioned: ['OpenAI', 'Codex', 'GitHub Copilot'],
    use_case: 'Code review automation',
    sentiment: 'frustrated',
    urgency: 'high',
    summary: 'Code generation inconsistency blocking automation project, comparing to Copilot',
    customer_name: 'DevTools Pro',
    arr_tier: 'mid_market',
    customer_health: 'at_risk',
    strategic_value: 'standard'
  },
  {
    id: 'demo-24',
    timestamp: new Date(Date.now() - 12 * 24 * 60 * 60 * 1000).toISOString(),
    source: 'support',
    raw_text: "Le Chat is losing conversation context after about 10 messages. Users report having to repeat information. This is frustrating for our customer support team who rely on longer conversations.",
    category: 'bug',
    features_mentioned: ['Le Chat', 'conversation context', 'memory'],
    competitors_mentioned: [],
    use_case: 'Customer support conversations',
    sentiment: 'frustrated',
    urgency: 'medium',
    summary: 'Le Chat losing conversation context after 10 messages, affecting support team',
    customer_name: 'SupportHub',
    arr_tier: 'mid_market',
    customer_health: 'healthy',
    strategic_value: 'standard'
  },
  {
    id: 'demo-25',
    timestamp: new Date(Date.now() - 16 * 24 * 60 * 60 * 1000).toISOString(),
    source: 'email',
    raw_text: "Function calling is failing silently. When the model decides not to call a function, we get no indication why. OpenAI provides reasoning. Makes debugging very difficult.",
    category: 'bug',
    features_mentioned: ['function calling', 'debugging', 'error handling'],
    competitors_mentioned: ['OpenAI'],
    use_case: 'AI agent development',
    sentiment: 'frustrated',
    urgency: 'medium',
    summary: 'Function calling fails silently without reasoning, unlike OpenAI',
    customer_name: 'AgentBuilder',
    arr_tier: 'mid_market',
    customer_health: 'healthy',
    strategic_value: 'standard'
  },
  {
    id: 'demo-26',
    timestamp: new Date(Date.now() - 19 * 24 * 60 * 60 * 1000).toISOString(),
    source: 'slack',
    raw_text: "API dashboard shows different usage numbers than our internal tracking. We've triple-checked our logging. The discrepancy is causing budget approval issues internally.",
    category: 'bug',
    features_mentioned: ['usage dashboard', 'billing', 'tracking'],
    competitors_mentioned: [],
    use_case: 'Usage monitoring and budgeting',
    sentiment: 'frustrated',
    urgency: 'medium',
    summary: 'API dashboard usage numbers dont match customer internal tracking',
    customer_name: 'TrackIt',
    arr_tier: 'mid_market',
    customer_health: 'healthy',
    strategic_value: 'standard'
  },

  // === COMPETITIVE INTEL (8 items) ===
  {
    id: 'demo-4',
    timestamp: new Date(Date.now() - 7 * 24 * 60 * 60 * 1000).toISOString(),
    source: 'email',
    raw_text: "Following up on our conversation about function calling. We're building an agent framework and need more reliable structured outputs. Currently using JSON mode but getting malformed responses ~5% of the time. Claude's tool use is more consistent. Can you share your roadmap for improving this?",
    category: 'competitive',
    features_mentioned: ['function calling', 'JSON mode', 'structured outputs', 'agents'],
    competitors_mentioned: ['Anthropic', 'Claude'],
    use_case: 'AI agent framework',
    sentiment: 'neutral',
    urgency: 'medium',
    summary: 'Customer comparing function calling reliability to Claude, needs improvement',
    customer_name: 'AgentStack',
    arr_tier: 'mid_market',
    customer_health: 'at_risk',
    strategic_value: 'standard'
  },
  {
    id: 'demo-27',
    timestamp: new Date(Date.now() - 2 * 24 * 60 * 60 * 1000).toISOString(),
    source: 'call_notes',
    raw_text: "Competitive deal alert: Customer is evaluating Claude 3.5 Sonnet against Mistral Large for their content generation platform. They say Claude has better creative writing capabilities. We need to highlight our strengths in speed and cost.",
    category: 'competitive',
    features_mentioned: ['content generation', 'creative writing', 'Mistral Large'],
    competitors_mentioned: ['Anthropic', 'Claude', 'Claude 3.5 Sonnet'],
    use_case: 'Content generation platform',
    sentiment: 'neutral',
    urgency: 'high',
    summary: 'Competitive deal vs Claude 3.5 Sonnet for content generation, Claude seen as better at creative writing',
    customer_name: 'ContentAI',
    arr_tier: 'enterprise',
    customer_health: 'at_risk',
    strategic_value: 'lighthouse'
  },
  {
    id: 'demo-28',
    timestamp: new Date(Date.now() - 6 * 24 * 60 * 60 * 1000).toISOString(),
    source: 'qbr',
    raw_text: "Customer mentioned they tested GPT-4o and were impressed with the multi-modal capabilities and speed. Currently using Mistral for text but considering OpenAI for anything involving images. Need to accelerate Pixtral roadmap.",
    category: 'competitive',
    features_mentioned: ['multi-modal', 'vision', 'Pixtral', 'speed'],
    competitors_mentioned: ['OpenAI', 'GPT-4o'],
    use_case: 'Multi-modal applications',
    sentiment: 'neutral',
    urgency: 'medium',
    summary: 'Customer impressed with GPT-4o multi-modal, considering OpenAI for image tasks',
    customer_name: 'VisionFirst',
    arr_tier: 'mid_market',
    customer_health: 'healthy',
    strategic_value: 'standard'
  },
  {
    id: 'demo-29',
    timestamp: new Date(Date.now() - 13 * 24 * 60 * 60 * 1000).toISOString(),
    source: 'email',
    raw_text: "Our security team completed their vendor assessment. Anthropic scored higher on their responsible AI practices documentation. Can you share more details about Mistral's AI safety measures and red teaming processes?",
    category: 'competitive',
    features_mentioned: ['AI safety', 'responsible AI', 'security assessment'],
    competitors_mentioned: ['Anthropic'],
    use_case: 'Enterprise security evaluation',
    sentiment: 'neutral',
    urgency: 'medium',
    summary: 'Security team ranks Anthropic higher on responsible AI documentation',
    customer_name: 'SecureBank',
    arr_tier: 'enterprise',
    customer_health: 'healthy',
    strategic_value: 'lighthouse'
  },
  {
    id: 'demo-30',
    timestamp: new Date(Date.now() - 17 * 24 * 60 * 60 * 1000).toISOString(),
    source: 'slack',
    raw_text: "Heads up - our ML team ran benchmarks and found Llama 3.1 70B performs comparably to Mistral Large on our use cases but at lower inference cost when self-hosted. Management is asking why we're paying for API access.",
    category: 'competitive',
    features_mentioned: ['benchmarks', 'self-hosting', 'inference cost'],
    competitors_mentioned: ['Meta', 'Llama', 'Llama 3.1'],
    use_case: 'General LLM workloads',
    sentiment: 'neutral',
    urgency: 'high',
    summary: 'ML team benchmarks show Llama 3.1 70B comparable at lower self-hosted cost',
    customer_name: 'MLOps Team',
    arr_tier: 'enterprise',
    customer_health: 'at_risk',
    strategic_value: 'lighthouse'
  },
  {
    id: 'demo-31',
    timestamp: new Date(Date.now() - 21 * 24 * 60 * 60 * 1000).toISOString(),
    source: 'call_notes',
    raw_text: "Customer demo'd Amazon Bedrock to their team. They liked the unified interface for multiple models. Asking if Mistral will have deeper AWS integration or a similar multi-model platform.",
    category: 'competitive',
    features_mentioned: ['AWS integration', 'Bedrock', 'multi-model platform'],
    competitors_mentioned: ['Amazon', 'AWS Bedrock'],
    use_case: 'Cloud infrastructure integration',
    sentiment: 'neutral',
    urgency: 'medium',
    summary: 'Customer interested in Amazon Bedrock unified interface, asking about AWS integration',
    customer_name: 'CloudNative Inc',
    arr_tier: 'enterprise',
    customer_health: 'healthy',
    strategic_value: 'standard'
  },
  {
    id: 'demo-32',
    timestamp: new Date(Date.now() - 24 * 24 * 60 * 60 * 1000).toISOString(),
    source: 'email',
    raw_text: "We evaluated Cohere for our search and retrieval use case. Their embedding models have better performance on our domain-specific content. Would consider Mistral if you had comparable embedding quality.",
    category: 'competitive',
    features_mentioned: ['embeddings', 'search', 'retrieval'],
    competitors_mentioned: ['Cohere'],
    use_case: 'Enterprise search',
    sentiment: 'neutral',
    urgency: 'low',
    summary: 'Customer found Cohere embeddings better for domain-specific search',
    customer_name: 'SearchFirst',
    arr_tier: 'mid_market',
    customer_health: 'healthy',
    strategic_value: 'standard'
  },
  {
    id: 'demo-33',
    timestamp: new Date(Date.now() - 26 * 24 * 60 * 60 * 1000).toISOString(),
    source: 'support',
    raw_text: "Google's Gemini Pro is offering very competitive pricing and their Vertex AI integration is seamless for us since we're on GCP. What's Mistral's GCP story? Currently feels like you're more AWS-focused.",
    category: 'competitive',
    features_mentioned: ['GCP integration', 'Vertex AI', 'pricing'],
    competitors_mentioned: ['Google', 'Gemini', 'Gemini Pro'],
    use_case: 'GCP-based infrastructure',
    sentiment: 'neutral',
    urgency: 'medium',
    summary: 'GCP customer comparing to Gemini Pro, feels Mistral is AWS-focused',
    customer_name: 'GCPFirst',
    arr_tier: 'mid_market',
    customer_health: 'healthy',
    strategic_value: 'standard'
  },

  // === PRICING CONCERNS (5 items) ===
  {
    id: 'demo-8',
    timestamp: new Date(Date.now() - 6 * 24 * 60 * 60 * 1000).toISOString(),
    source: 'call_notes',
    raw_text: "Pricing discussion: Customer currently on $50K/year plan. Usage has grown 3x but they're hesitant to upgrade because committed pricing is unclear. They want annual commits with volume discounts. Mentioned that OpenAI offered them a 30% discount for annual commitment.",
    category: 'pricing',
    features_mentioned: ['volume discounts', 'annual commits', 'enterprise pricing'],
    competitors_mentioned: ['OpenAI'],
    use_case: 'General API usage',
    sentiment: 'neutral',
    urgency: 'high',
    summary: 'Customer wants volume discounts and annual commits, OpenAI offering 30% discount',
    customer_name: 'StartupAI',
    arr_tier: 'mid_market',
    customer_health: 'healthy',
    strategic_value: 'standard'
  },
  {
    id: 'demo-34',
    timestamp: new Date(Date.now() - 3 * 24 * 60 * 60 * 1000).toISOString(),
    source: 'email',
    raw_text: "Our finance team is pushing back on the current pricing. At our volume (500M tokens/month), the per-token cost is higher than what Anthropic quoted us. Need to discuss enterprise rates or we'll have to reconsider.",
    category: 'pricing',
    features_mentioned: ['enterprise pricing', 'volume pricing', 'per-token cost'],
    competitors_mentioned: ['Anthropic'],
    use_case: 'High-volume production',
    sentiment: 'negative',
    urgency: 'high',
    summary: 'Finance pushing back on pricing at 500M tokens/month, Anthropic quoted lower',
    customer_name: 'HighVolume Corp',
    arr_tier: 'enterprise',
    customer_health: 'at_risk',
    strategic_value: 'lighthouse'
  },
  {
    id: 'demo-35',
    timestamp: new Date(Date.now() - 10 * 24 * 60 * 60 * 1000).toISOString(),
    source: 'slack',
    raw_text: "Can we get clarity on pricing for Mistral Small vs Mistral Large? The documentation is confusing. We want to optimize costs by using Small where possible but need clear guidance on capability differences.",
    category: 'pricing',
    features_mentioned: ['Mistral Small', 'Mistral Large', 'cost optimization'],
    competitors_mentioned: [],
    use_case: 'Cost-optimized deployment',
    sentiment: 'neutral',
    urgency: 'medium',
    summary: 'Customer confused about Small vs Large pricing and capability differences',
    customer_name: 'OptimizeAI',
    arr_tier: 'mid_market',
    customer_health: 'healthy',
    strategic_value: 'standard'
  },
  {
    id: 'demo-36',
    timestamp: new Date(Date.now() - 15 * 24 * 60 * 60 * 1000).toISOString(),
    source: 'qbr',
    raw_text: "Startup customer on credits is worried about what happens when credits run out. They're pre-revenue and need predictable costs. Asking about startup programs or extended credit options.",
    category: 'pricing',
    features_mentioned: ['startup credits', 'startup program', 'predictable costs'],
    competitors_mentioned: [],
    use_case: 'Early-stage startup',
    sentiment: 'neutral',
    urgency: 'medium',
    summary: 'Pre-revenue startup worried about costs when credits expire, needs startup program',
    customer_name: 'EarlyStage AI',
    arr_tier: 'smb',
    customer_health: 'healthy',
    strategic_value: 'standard'
  },
  {
    id: 'demo-37',
    timestamp: new Date(Date.now() - 20 * 24 * 60 * 60 * 1000).toISOString(),
    source: 'email',
    raw_text: "We're scaling our usage from $10K to potentially $100K/month. At what point do we qualify for enterprise pricing? The current pay-as-you-go model doesn't work for our budgeting process. Need committed pricing.",
    category: 'pricing',
    features_mentioned: ['enterprise pricing', 'committed pricing', 'scaling'],
    competitors_mentioned: [],
    use_case: 'Scaling production deployment',
    sentiment: 'neutral',
    urgency: 'high',
    summary: 'Customer scaling 10x wants enterprise committed pricing for budgeting',
    customer_name: 'ScaleUp AI',
    arr_tier: 'mid_market',
    customer_health: 'healthy',
    strategic_value: 'lighthouse'
  },

  // === PRAISE (7 items) ===
  {
    id: 'demo-3',
    timestamp: new Date(Date.now() - 1 * 24 * 60 * 60 * 1000).toISOString(),
    source: 'nps',
    raw_text: "NPS: 9. Love the model quality and the team responsiveness. Mistral Small is perfect for our classification tasks - fast and accurate. Only wish: better documentation for fine-tuning. Had to figure out a lot through trial and error.",
    category: 'praise',
    features_mentioned: ['Mistral Small', 'fine-tuning', 'documentation'],
    competitors_mentioned: [],
    use_case: 'Text classification',
    sentiment: 'positive',
    urgency: 'low',
    summary: 'Happy customer loves Mistral Small, wants better fine-tuning docs',
    customer_name: 'DataFlow Inc',
    arr_tier: 'mid_market',
    customer_health: 'healthy',
    strategic_value: 'standard'
  },
  {
    id: 'demo-10',
    timestamp: new Date(Date.now() - 12 * 24 * 60 * 60 * 1000).toISOString(),
    source: 'email',
    raw_text: "Just wanted to share some positive feedback - we switched from GPT-4 to Mistral Large for our customer support chatbot and saw 40% cost reduction with comparable quality. Response times also improved. Our CTO is very happy. Looking forward to exploring more use cases with your team.",
    category: 'praise',
    features_mentioned: ['Mistral Large', 'customer support', 'chatbot', 'cost efficiency'],
    competitors_mentioned: ['OpenAI', 'GPT-4'],
    use_case: 'Customer support chatbot',
    sentiment: 'positive',
    urgency: 'low',
    summary: 'Customer saved 40% switching from GPT-4 to Mistral Large with better response times',
    customer_name: 'SupportHub',
    arr_tier: 'mid_market',
    customer_health: 'healthy',
    strategic_value: 'standard'
  },
  {
    id: 'demo-38',
    timestamp: new Date(Date.now() - 2 * 24 * 60 * 60 * 1000).toISOString(),
    source: 'nps',
    raw_text: "NPS: 10. Mistral has been a game-changer for our startup. The price-to-performance ratio is unbeatable. We've built our entire product on Mistral Small and couldn't be happier. The API is clean and the latency is excellent.",
    category: 'praise',
    features_mentioned: ['Mistral Small', 'API', 'latency', 'price-performance'],
    competitors_mentioned: [],
    use_case: 'Startup product development',
    sentiment: 'positive',
    urgency: 'low',
    summary: 'Startup gives NPS 10, built entire product on Mistral Small, loves price-performance',
    customer_name: 'InnovateTech',
    arr_tier: 'smb',
    customer_health: 'healthy',
    strategic_value: 'standard'
  },
  {
    id: 'demo-39',
    timestamp: new Date(Date.now() - 8 * 24 * 60 * 60 * 1000).toISOString(),
    source: 'slack',
    raw_text: "Just want to give a shoutout to the Mistral team. Our support ticket was resolved in under 2 hours on a Sunday. That level of responsiveness is rare. Keep it up!",
    category: 'praise',
    features_mentioned: ['support', 'responsiveness'],
    competitors_mentioned: [],
    use_case: 'Customer support experience',
    sentiment: 'positive',
    urgency: 'low',
    summary: 'Customer praises support team for 2-hour Sunday response time',
    customer_name: 'HappyCustomer',
    arr_tier: 'mid_market',
    customer_health: 'healthy',
    strategic_value: 'standard'
  },
  {
    id: 'demo-40',
    timestamp: new Date(Date.now() - 11 * 24 * 60 * 60 * 1000).toISOString(),
    source: 'email',
    raw_text: "Our engineering team wanted me to pass along that they love working with the Mistral API. The documentation is clear, the SDKs are well-maintained, and the error messages are actually helpful. Small things that make a big difference.",
    category: 'praise',
    features_mentioned: ['API', 'documentation', 'SDKs', 'developer experience'],
    competitors_mentioned: [],
    use_case: 'Developer integration',
    sentiment: 'positive',
    urgency: 'low',
    summary: 'Engineering team praises API documentation, SDKs, and helpful error messages',
    customer_name: 'DevTeam Inc',
    arr_tier: 'mid_market',
    customer_health: 'healthy',
    strategic_value: 'standard'
  },
  {
    id: 'demo-41',
    timestamp: new Date(Date.now() - 14 * 24 * 60 * 60 * 1000).toISOString(),
    source: 'qbr',
    raw_text: "Customer shared that they've expanded Mistral usage to 3 additional teams since our last QBR. Originally just the ML team, now marketing, customer success, and product are all using Le Chat. Great organic growth.",
    category: 'praise',
    features_mentioned: ['Le Chat', 'organic growth', 'team expansion'],
    competitors_mentioned: [],
    use_case: 'Cross-team AI adoption',
    sentiment: 'positive',
    urgency: 'low',
    summary: 'Customer expanded from 1 to 4 teams using Mistral/Le Chat organically',
    customer_name: 'GrowthCorp',
    arr_tier: 'enterprise',
    customer_health: 'healthy',
    strategic_value: 'lighthouse'
  },
  {
    id: 'demo-42',
    timestamp: new Date(Date.now() - 18 * 24 * 60 * 60 * 1000).toISOString(),
    source: 'nps',
    raw_text: "NPS: 9. The EU data residency option was the deciding factor for us. As a German company, GDPR compliance is non-negotiable. Mistral being a European company gives us confidence. Performance is excellent too.",
    category: 'praise',
    features_mentioned: ['EU data residency', 'GDPR', 'compliance'],
    competitors_mentioned: [],
    use_case: 'European enterprise deployment',
    sentiment: 'positive',
    urgency: 'low',
    summary: 'German customer chose Mistral for EU data residency and GDPR compliance',
    customer_name: 'DeutscheTech',
    arr_tier: 'enterprise',
    customer_health: 'healthy',
    strategic_value: 'lighthouse'
  },

  // === CHURN SIGNALS (5 items) ===
  {
    id: 'demo-5',
    timestamp: new Date(Date.now() - 3 * 24 * 60 * 60 * 1000).toISOString(),
    source: 'slack',
    raw_text: "Hey team, quick heads up - we're evaluating whether to renew next quarter. The new Llama 3.1 405B is showing competitive results in our benchmarks and it's open source. Our ML team is pushing to self-host. Can we discuss what Mistral offers that justifies the premium?",
    category: 'churn_signal',
    features_mentioned: ['self-hosting', 'open source'],
    competitors_mentioned: ['Meta', 'Llama'],
    use_case: 'General LLM workloads',
    sentiment: 'negative',
    urgency: 'high',
    summary: 'Customer considering switching to Llama 3.1 for self-hosting, renewal at risk',
    customer_name: 'CloudScale',
    arr_tier: 'enterprise',
    customer_health: 'at_risk',
    strategic_value: 'lighthouse'
  },
  {
    id: 'demo-9',
    timestamp: new Date(Date.now() - 8 * 24 * 60 * 60 * 1000).toISOString(),
    source: 'nps',
    raw_text: "NPS: 4. Disappointed with the EU data residency situation. We're a German healthcare company and GDPR compliance is non-negotiable. You promised EU hosting 6 months ago but still nothing. We may need to evaluate alternatives if this doesn't ship soon.",
    category: 'churn_signal',
    features_mentioned: ['EU data residency', 'GDPR', 'compliance', 'healthcare'],
    competitors_mentioned: [],
    use_case: 'Healthcare data processing',
    sentiment: 'frustrated',
    urgency: 'critical',
    summary: 'Healthcare customer frustrated about delayed EU data residency, considering alternatives',
    customer_name: 'MedTech GmbH',
    arr_tier: 'enterprise',
    customer_health: 'at_risk',
    strategic_value: 'lighthouse'
  },
  {
    id: 'demo-43',
    timestamp: new Date(Date.now() - 1 * 24 * 60 * 60 * 1000).toISOString(),
    source: 'email',
    raw_text: "We need to have a serious conversation about our contract. The reliability issues over the past month have cost us real money in SLA breaches to our own customers. If we can't get guaranteed uptime, we'll need to find a more reliable provider.",
    category: 'churn_signal',
    features_mentioned: ['reliability', 'SLA', 'uptime'],
    competitors_mentioned: [],
    use_case: 'Production SLA-bound services',
    sentiment: 'frustrated',
    urgency: 'critical',
    summary: 'Customer threatening to leave due to reliability issues causing SLA breaches',
    customer_name: 'SLAFirst',
    arr_tier: 'enterprise',
    customer_health: 'at_risk',
    strategic_value: 'lighthouse'
  },
  {
    id: 'demo-44',
    timestamp: new Date(Date.now() - 5 * 24 * 60 * 60 * 1000).toISOString(),
    source: 'call_notes',
    raw_text: "Difficult call with customer. Their new CTO is an ex-Googler and pushing hard to move everything to Vertex AI and Gemini. Current champion (VP Eng) is trying to defend Mistral but losing the internal battle. May need executive engagement.",
    category: 'churn_signal',
    features_mentioned: ['Vertex AI', 'executive engagement'],
    competitors_mentioned: ['Google', 'Gemini'],
    use_case: 'Enterprise AI platform',
    sentiment: 'negative',
    urgency: 'high',
    summary: 'New CTO pushing for Google/Gemini, internal champion losing battle, needs exec engagement',
    customer_name: 'EnterpriseAI',
    arr_tier: 'enterprise',
    customer_health: 'at_risk',
    strategic_value: 'lighthouse'
  },
  {
    id: 'demo-45',
    timestamp: new Date(Date.now() - 9 * 24 * 60 * 60 * 1000).toISOString(),
    source: 'qbr',
    raw_text: "Customer informed us they're pausing expansion plans. Budget cuts mean they can't increase spend this year. They'll maintain current usage but won't be growing. Asked about downgrade options to reduce current spend by 30%.",
    category: 'churn_signal',
    features_mentioned: ['budget cuts', 'downgrade', 'cost reduction'],
    competitors_mentioned: [],
    use_case: 'Budget-constrained enterprise',
    sentiment: 'negative',
    urgency: 'medium',
    summary: 'Customer pausing expansion due to budget cuts, asking about 30% downgrade options',
    customer_name: 'BudgetCut Corp',
    arr_tier: 'enterprise',
    customer_health: 'at_risk',
    strategic_value: 'standard'
  }
];

export const DEMO_WEB_SIGNALS = [
  {
    id: 'web-demo-1',
    timestamp: new Date(Date.now() - 1 * 24 * 60 * 60 * 1000).toISOString(),
    source: 'web',
    web_source: 'Reddit',
    web_url: 'https://reddit.com/r/LocalLLaMA/example',
    raw_text: "Just tested Mistral Large 2 against GPT-4 and Claude 3.5 on our internal benchmarks. Mistral is winning on speed and cost, but Claude still edges out on complex reasoning tasks. For most production use cases though, Mistral is the sweet spot.",
    title: 'Mistral Large 2 vs GPT-4 vs Claude 3.5 - Real benchmarks',
    search_query: 'Mistral AI comparison',
    theme: 'Model comparison and benchmarking',
    sentiment: 'positive',
    competitors_mentioned: ['OpenAI', 'GPT-4', 'Anthropic', 'Claude'],
    key_points: ['Mistral winning on speed and cost', 'Claude better at complex reasoning', 'Mistral best for production'],
    relevance: 'high'
  },
  {
    id: 'web-demo-2',
    timestamp: new Date(Date.now() - 3 * 24 * 60 * 60 * 1000).toISOString(),
    source: 'web',
    web_source: 'Hacker News',
    web_url: 'https://news.ycombinator.com/example',
    raw_text: "Le Chat is surprisingly good. Been using it as my daily driver for coding and it's faster than ChatGPT. The web search integration is seamless. Only complaint: wish it had better code execution like Claude Artifacts.",
    title: 'Le Chat Review - A viable ChatGPT alternative?',
    search_query: 'Le Chat Mistral',
    theme: 'Le Chat user experience',
    sentiment: 'positive',
    competitors_mentioned: ['OpenAI', 'ChatGPT', 'Anthropic', 'Claude'],
    key_points: ['Faster than ChatGPT', 'Good web search', 'Missing code execution feature'],
    relevance: 'high'
  },
  {
    id: 'web-demo-3',
    timestamp: new Date(Date.now() - 5 * 24 * 60 * 60 * 1000).toISOString(),
    source: 'web',
    web_source: 'Reddit',
    web_url: 'https://reddit.com/r/MachineLearning/example',
    raw_text: "Anyone else having issues with Mistral API rate limits? Getting throttled way more aggressively than the docs suggest. Support has been slow to respond. Considering moving our dev workloads back to OpenAI.",
    title: 'Mistral API rate limiting issues',
    search_query: 'Mistral API',
    theme: 'API reliability concerns',
    sentiment: 'negative',
    competitors_mentioned: ['OpenAI'],
    key_points: ['Rate limit issues', 'Slow support response', 'Considering switching'],
    relevance: 'high'
  },
  {
    id: 'web-demo-4',
    timestamp: new Date(Date.now() - 7 * 24 * 60 * 60 * 1000).toISOString(),
    source: 'web',
    web_source: 'Dev.to',
    web_url: 'https://dev.to/example',
    raw_text: "Tutorial: How we migrated from OpenAI to Mistral and saved 60% on our AI costs. The models are comparable for our summarization use case, and the European data residency was a bonus for GDPR compliance.",
    title: 'Migrating from OpenAI to Mistral: A Cost Analysis',
    search_query: 'Mistral vs OpenAI',
    theme: 'Cost comparison and migration',
    sentiment: 'positive',
    competitors_mentioned: ['OpenAI'],
    key_points: ['60% cost savings', 'Comparable quality', 'GDPR compliance benefit'],
    relevance: 'high'
  },
  {
    id: 'web-demo-5',
    timestamp: new Date(Date.now() - 2 * 24 * 60 * 60 * 1000).toISOString(),
    source: 'web',
    web_source: 'Twitter/X',
    web_url: 'https://x.com/example',
    raw_text: "Pixtral is impressive for document understanding. Just processed 1000 invoices with 98% accuracy. The vision capabilities are finally on par with GPT-4V but at a fraction of the cost.",
    title: 'Pixtral invoice processing results',
    search_query: 'Pixtral Mistral vision',
    theme: 'Vision model capabilities',
    sentiment: 'positive',
    competitors_mentioned: ['OpenAI', 'GPT-4V'],
    key_points: ['98% accuracy on invoices', 'On par with GPT-4V', 'Lower cost'],
    relevance: 'high'
  },
  {
    id: 'web-demo-6',
    timestamp: new Date(Date.now() - 4 * 24 * 60 * 60 * 1000).toISOString(),
    source: 'web',
    web_source: 'Hacker News',
    web_url: 'https://news.ycombinator.com/example2',
    raw_text: "Hot take: Mistral is becoming the go-to for startups who want GPT-4 level quality without the OpenAI lock-in. The open weights models mean you can always self-host if needed.",
    title: 'Why startups are choosing Mistral over OpenAI',
    search_query: 'Mistral startup',
    theme: 'Startup adoption trends',
    sentiment: 'positive',
    competitors_mentioned: ['OpenAI', 'GPT-4'],
    key_points: ['GPT-4 level quality', 'No vendor lock-in', 'Self-hosting option'],
    relevance: 'high'
  }
];

export function loadDemoData() {
  localStorage.setItem('voc_feedback_items', JSON.stringify(DEMO_FEEDBACK));
  localStorage.setItem('voc_web_signals', JSON.stringify(DEMO_WEB_SIGNALS));
  localStorage.setItem('voc_product_briefs', JSON.stringify([]));
  localStorage.setItem('voc_demo_loaded', 'true');

  return {
    feedbackCount: DEMO_FEEDBACK.length,
    signalsCount: DEMO_WEB_SIGNALS.length
  };
}

export function clearAllData() {
  localStorage.removeItem('voc_feedback_items');
  localStorage.removeItem('voc_web_signals');
  localStorage.removeItem('voc_product_briefs');
  localStorage.removeItem('voc_demo_loaded');
}

export function shouldAutoLoadDemo() {
  return !localStorage.getItem('voc_demo_loaded');
}
